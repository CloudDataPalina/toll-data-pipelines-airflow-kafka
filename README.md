# ğŸš¦ Toll Data Pipelines (Airflow & Kafka)

![Airflow](https://img.shields.io/badge/Apache-Airflow-blue?logo=apacheairflow&logoColor=white)
![Kafka](https://img.shields.io/badge/Apache-Kafka-black?logo=apachekafka&logoColor=white)
![Python](https://img.shields.io/badge/Python-3.x-blue?logo=python&logoColor=white)
![Bash](https://img.shields.io/badge/Bash-Shell_Scripts-121011?logo=gnu-bash&logoColor=white)
![MySQL](https://img.shields.io/badge/MySQL-Database-orange?logo=mysql&logoColor=white)
![Status](https://img.shields.io/badge/Status-Completed-brightgreen)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

---

## âœ… Project Status
This project is fully implemented and tested in a controlled environment.  
It demonstrates **batch and streaming data pipelines** using Apache Airflow and Apache Kafka.
Open for future improvements and enhancements.

---

## ğŸ“Œ Project Overview

This project implements an end-to-end **data engineering platform** for processing **traffic toll data** using two complementary approaches:

### ğŸ”¹ Batch Processing (Apache Airflow)
- Extracts data from multiple file formats:
  - CSV
  - TSV
  - Fixed-width text files
- Transforms and consolidates data into a unified dataset
- Demonstrates two ETL implementations:
  - **BashOperator-based ETL** (shell tools: cut, paste, tr)
  - **PythonOperator-based ETL** (Python, requests, csv)

### ğŸ”¹ Streaming Processing (Apache Kafka)
- Consumes real-time vehicle passage events from a Kafka topic
- Inserts streaming data into a MySQL database table
- Demonstrates Kafka â†’ Database ingestion pipeline

This project reflects **real-world data engineering scenarios** where batch and streaming pipelines coexist.

---

## ğŸ“‚ Project Structure

```text
toll-data-pipelines-airflow-kafka/
â”œâ”€â”€ README.md
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ dags/
â”‚       â”œâ”€â”€ etl_toll_bashoperator.py      â† Batch ETL using BashOperator
â”‚       â””â”€â”€ etl_toll_pythonoperator.py    â† Batch ETL using PythonOperator
â”‚
â”œâ”€â”€ streaming/
â”‚   â””â”€â”€ kafka_to_mysql_consumer.py        â† Kafka consumer (streaming ETL)
â”‚
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ init_mysql.sql                    â† MySQL schema initialization
â”‚
â””â”€â”€ LICENSE
```
---
## ğŸ› ï¸ Skills & Tools
- Apache Airflow â€” workflow orchestration and DAG scheduling
- Bash â€” shell-based ETL processing (cut, paste, tr, tar)
- Python â€” ETL logic, HTTP requests, CSV parsing
- Apache Kafka â€” real-time data streaming
- MySQL â€” persistent storage for streaming data
- Linux CLI â€” data engineering fundamentals

---

## ğŸš€ How to Run
### 1ï¸âƒ£ Batch ETL with Apache Airflow
Two DAGs are available in airflow/dags/:
- etl_toll_bashoperator.py
- etl_toll_pythonoperator.py

#### Steps:
1. Start Apache Airflow
2. Copy DAG files into the `$AIRFLOW_HOME/dags` directory
3. Enable the DAG in the Airflow UI
4. Trigger the DAG manually or via schedule
5. Verify generated files created by the DAG tasks

### 2ï¸âƒ£ Streaming ETL with Kafka â†’ MySQL
#### Prepare MySQL
Ensure Kafka broker and MySQL are running before starting the consumer.
The consumer runs continuously and processes messages in real time.
Execute the SQL script:

```sql
source sql/init_mysql.sql;
```

This creates:
- database: tolldata
- table: livetolldata

#### Run Kafka Consumer
```bash
python streaming/kafka_to_mysql_consumer.py

```
The consumer:
- reads messages from Kafka topic `toll`
- parses vehicle events
- inserts streaming data into a MySQL database table for persistent storage.

Note: Kafka broker and MySQL are expected to be available on localhost
(e.g. via Docker, local installation, or Codespaces services).
Kafka and MySQL were validated using Docker Compose in GitHub Codespaces (local development setup).

---

## âœ… Example Output (Streaming Verification)

After producing a test message to the Kafka topic and running the consumer,
the data is successfully inserted into the MySQL database.

#### Sample Kafka Message
```text
Sat Jun 01 12:00:00 2024,101,car,7
```

#### MySQL Table Output
```sql
SELECT * FROM livetolldata;
```

```text
+---------------------+------------+--------------+----------------+
| timestamp           | vehicle_id | vehicle_type | toll_plaza_id |
+---------------------+------------+--------------+----------------+
| 2024-06-01 12:00:00 | 101        | car          | 7              |
+---------------------+------------+--------------+----------------+
```
--- 

## ğŸ”„ Data Flow Diagram
```text
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ CSV / TSV /  â”‚
                â”‚ Fixed-width  â”‚
                â”‚   files      â”‚
                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                 Apache Airflow
                       â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                       â”‚
   BashOperator ETL        PythonOperator ETL
           â”‚                       â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
              transformed_data.csv
                       â”‚
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                       â”‚
                   Kafka Topic
                       â”‚
              Kafka Consumer (Python)
                       â”‚
                    MySQL DB
                 livetolldata table
```
## ğŸ” Key Engineering Highlights
- Same ETL logic implemented using **two different orchestration styles**
- Demonstrates **batch vs streaming** processing
- Clear separation of concerns (Airflow / Kafka / SQL)
- Production-oriented project structure
- Readable, maintainable, and extensible codebase

## ğŸ“ Summary
- Built batch ETL pipelines with Apache Airflow
- Implemented streaming ingestion with Apache Kafka
- Stored real-time data into MySQL
- Demonstrated core Data Engineering concepts end-to-end

## ğŸ“œ License

This project is released under the **MIT License** â€” see [`LICENSE`](LICENSE).  

***Enjoy and have a great data brew*** â˜•ï¸ğŸ™‚

---

## ğŸ‘©â€ğŸ’» Author

**Palina Krasiuk**  
Aspiring Cloud Data Engineer | ex-Senior Accountant  
[LinkedIn](https://www.linkedin.com/in/palina-krasiuk-954404372/) â€¢ [GitHub Portfolio](https://github.com/CloudDataPalina)


  
